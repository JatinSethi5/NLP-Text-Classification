{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tokenizers import Tokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/soumyekumar/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/soumyekumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/soumyekumar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data = pd.read_csv(\"data/output_chunk_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data_98 = og_data.sample(frac = 0.98) # work data for assignment\n",
    "og_data_02 = og_data.drop(og_data_98.index) # test data to be used by prof\n",
    "#og_data_02.to_csv('data/og_data_02.csv', index=False)  # already created csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>humanInterest</td>\n",
       "      <td>five lancaster county schools have earned reco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5894</th>\n",
       "      <td>social</td>\n",
       "      <td>appleton ap when doug klemp and denise schuelk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>lifestyle</td>\n",
       "      <td>an aerial photograph of the big buddha and kat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>labour</td>\n",
       "      <td>walk4fitness program concludes . community new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>sport</td>\n",
       "      <td>wembley hosted all of england s group d matche...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text\n",
       "3114  humanInterest  five lancaster county schools have earned reco...\n",
       "5894         social  appleton ap when doug klemp and denise schuelk...\n",
       "3771      lifestyle  an aerial photograph of the big buddha and kat...\n",
       "3382         labour  walk4fitness program concludes . community new...\n",
       "6042          sport  wembley hosted all of england s group d matche..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_data_98.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    144\n",
       "text        144\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_data_02.count() # check if 2% is kept aside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3114</th>\n",
       "      <td>humanInterest</td>\n",
       "      <td>five lancaster county schools have earned reco...</td>\n",
       "      <td>five lancast counti school earn recognit recip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5894</th>\n",
       "      <td>social</td>\n",
       "      <td>appleton ap when doug klemp and denise schuelk...</td>\n",
       "      <td>appleton ap doug klemp denis schuelk start dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3771</th>\n",
       "      <td>lifestyle</td>\n",
       "      <td>an aerial photograph of the big buddha and kat...</td>\n",
       "      <td>aerial photograph big buddha kata beach behind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3382</th>\n",
       "      <td>labour</td>\n",
       "      <td>walk4fitness program concludes . community new...</td>\n",
       "      <td>walkfit program conclud  communiti news  june ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6042</th>\n",
       "      <td>sport</td>\n",
       "      <td>wembley hosted all of england s group d matche...</td>\n",
       "      <td>wembley host england group match see three lio...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text  \\\n",
       "3114  humanInterest  five lancaster county schools have earned reco...   \n",
       "5894         social  appleton ap when doug klemp and denise schuelk...   \n",
       "3771      lifestyle  an aerial photograph of the big buddha and kat...   \n",
       "3382         labour  walk4fitness program concludes . community new...   \n",
       "6042          sport  wembley hosted all of england s group d matche...   \n",
       "\n",
       "                                             clean_text  \n",
       "3114  five lancast counti school earn recognit recip...  \n",
       "5894  appleton ap doug klemp denis schuelk start dat...  \n",
       "3771  aerial photograph big buddha kata beach behind...  \n",
       "3382  walkfit program conclud  communiti news  june ...  \n",
       "6042  wembley host england group match see three lio...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    # call stop words and remove them\n",
    "    stop_words = stopwords.words('english') \n",
    "    removed_stopwords_text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    # Perform stemming\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    return ' '.join(stemmer.stem(word) for word in removed_stopwords_text.split(' '))\n",
    "\n",
    "\n",
    "og_data_98[\"clean_text\"] = og_data_98[\"text\"].apply(clean_text)\n",
    "og_data_98.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB with Bag of Words accuracy: 0.683\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_nb= og_data_98[\"clean_text\"]\n",
    "y_nb= og_data_98[\"category\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test= train_test_split(X_nb,y_nb,test_size=.2,random_state=42)\n",
    "\n",
    "# Multinomial Naive Bayes with Bag of Words\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"MultinomialNB with Bag of Words accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:[0.66284014 0.67431973 0.66921769]\n",
      "Mean Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "cv_scores = cross_val_score(model, X_nb, y_nb, cv=StratifiedKFold(n_splits=3, shuffle=True), scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-Validation Scores:{cv_scores}\")\n",
    "\n",
    "print(f\"Mean Accuracy: {np.mean(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 Accuracy: 0.9023526077097506\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_logistic = le.fit_transform(og_data_98['category'])\n",
    "X_logistic = og_data_98['clean_text']\n",
    "\n",
    "# Create pipeline\n",
    "model2 = make_pipeline(\n",
    "    TfidfVectorizer(max_features=5000),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "# Train\n",
    "model2.fit(X_logistic, y_logistic)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Model 2 Accuracy:\", model2.score(X_logistic, y_logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 78ms/step - accuracy: 0.0833 - loss: 2.8345 - val_accuracy: 0.2330 - val_loss: 2.3925\n",
      "Epoch 2/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.3044 - loss: 2.1399 - val_accuracy: 0.3017 - val_loss: 2.2110\n",
      "Epoch 3/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 106ms/step - accuracy: 0.4438 - loss: 1.7426 - val_accuracy: 0.4256 - val_loss: 1.9236\n",
      "Epoch 4/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 110ms/step - accuracy: 0.6326 - loss: 1.2248 - val_accuracy: 0.4688 - val_loss: 1.8922\n",
      "Epoch 5/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 111ms/step - accuracy: 0.7632 - loss: 0.8393 - val_accuracy: 0.5241 - val_loss: 1.8684\n",
      "Epoch 6/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 114ms/step - accuracy: 0.8362 - loss: 0.5807 - val_accuracy: 0.4703 - val_loss: 2.1263\n",
      "Epoch 7/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m100s\u001b[0m 565ms/step - accuracy: 0.8411 - loss: 0.5390 - val_accuracy: 0.5319 - val_loss: 2.0953\n",
      "Epoch 8/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 112ms/step - accuracy: 0.9364 - loss: 0.2494 - val_accuracy: 0.5262 - val_loss: 2.2134\n",
      "Epoch 9/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 112ms/step - accuracy: 0.9636 - loss: 0.1575 - val_accuracy: 0.5418 - val_loss: 2.4550\n",
      "Epoch 10/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 110ms/step - accuracy: 0.9751 - loss: 0.1149 - val_accuracy: 0.5042 - val_loss: 2.6103\n",
      "Epoch 11/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 112ms/step - accuracy: 0.9817 - loss: 0.0952 - val_accuracy: 0.5234 - val_loss: 2.7207\n",
      "Epoch 12/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 114ms/step - accuracy: 0.9854 - loss: 0.0652 - val_accuracy: 0.5382 - val_loss: 2.9137\n",
      "Epoch 13/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 116ms/step - accuracy: 0.9738 - loss: 0.1029 - val_accuracy: 0.5198 - val_loss: 2.8823\n",
      "Epoch 14/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m320s\u001b[0m 2s/step - accuracy: 0.9889 - loss: 0.0400 - val_accuracy: 0.5149 - val_loss: 3.1357\n",
      "Epoch 15/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 113ms/step - accuracy: 0.9978 - loss: 0.0244 - val_accuracy: 0.5255 - val_loss: 3.1456\n",
      "Epoch 16/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 115ms/step - accuracy: 0.9986 - loss: 0.0096 - val_accuracy: 0.5375 - val_loss: 3.1466\n",
      "Epoch 17/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 113ms/step - accuracy: 0.9986 - loss: 0.0060 - val_accuracy: 0.5446 - val_loss: 3.2509\n",
      "Epoch 18/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 112ms/step - accuracy: 0.9989 - loss: 0.0054 - val_accuracy: 0.5496 - val_loss: 3.2882\n",
      "Epoch 19/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 113ms/step - accuracy: 0.9981 - loss: 0.0079 - val_accuracy: 0.5418 - val_loss: 3.3877\n",
      "Epoch 20/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 117ms/step - accuracy: 0.9988 - loss: 0.0052 - val_accuracy: 0.5319 - val_loss: 3.3949\n",
      "Model 3 Validation Accuracy: 0.5495750904083252\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(og_data_98['clean_text'])\n",
    "tokenizer.num_words = 5000\n",
    "sequences = tokenizer.texts_to_sequences(og_data_98['clean_text'])\n",
    "X_lstm = pad_sequences(sequences, maxlen=200)\n",
    "y_lstm= le.fit_transform(og_data_98['category'])\n",
    "\n",
    "\n",
    "# Build model\n",
    "model3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(5000, 128),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model3.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model3.fit(\n",
    "    X_lstm, y_lstm,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Model 3 Validation Accuracy:\", max(history.history['val_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels = og_data_98['category'] # Add your actual categories\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "# Save the trained label encoder\n",
    "pickle.dump(label_encoder, open('label_encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import joblib\n",
    "from tensorflow.keras.models import save_model,load_model \n",
    "\n",
    "\n",
    "# Model 1: Original Model (Naive Bayes)\n",
    "joblib.dump(model, 'NaiveBayes.pkl')\n",
    "\n",
    "# Model 2: Logistic Regression\n",
    "joblib.dump(model2, 'logreg_model.pkl')\n",
    "pickle.dump(model2, open('model.pkl','wb'))\n",
    "\n",
    "\n",
    "# Model 3: LSTM\n",
    "model3.save('lstm_model.h5')\n",
    "\n",
    "\n",
    "# Loading function for all models\n",
    "def load_all_models():\n",
    "    model1 = joblib.load('original_model.pkl')\n",
    "    model2 = joblib.load('logreg_model.pkl')\n",
    "    model3 = load_model('lstm_model.h5')\n",
    "    \n",
    "    return model1, model2, model3\n",
    "\n",
    "# Prediction function for all models\n",
    "def predict_all_models(text, model1, model2, model3):\n",
    "    cleaned = clean_text(text)\n",
    "    \n",
    "    # Model 1 (Naive Bayes)\n",
    "    pred1 = model1.predict([cleaned])[0]\n",
    "    \n",
    "    # Model 2 (Logistic Regression)\n",
    "    pred2 = model2.predict([cleaned])[0]\n",
    "    \n",
    "    # Model 3 (LSTM)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned])\n",
    "    padded = pad_sequences(sequence, maxlen=200)\n",
    "    pred3 = model3.predict(padded).argmax(axis=1)[0]\n",
    "    \n",
    "    return {\n",
    "        'NaiveBayes': le.inverse_transform([pred1])[0],\n",
    "        'Logistic Regression': le.inverse_transform([pred2])[0],\n",
    "        'LSTM': le.inverse_transform([pred3])[0]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MultinomialNB from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 125ms/step\n",
      "Predictions: {'NaiveBayes': 'arts', 'Logistic Regression': 'arts', 'LSTM': 'social'}\n"
     ]
    }
   ],
   "source": [
    "# Load models once at startup\n",
    "model1, model2, model3= load_all_models()\n",
    "\n",
    "# Make predictions\n",
    "sample_text = \"Archaeologists discover ancient temple ruins under city center\"\n",
    "predictions = predict_all_models(sample_text, model1, model2, model3)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
