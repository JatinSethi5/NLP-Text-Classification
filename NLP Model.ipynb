{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tokenizers import Tokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/soumyekumar/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/soumyekumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/soumyekumar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data = pd.read_csv(\"data/output_chunk_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data_98 = og_data.sample(frac = 0.98) # work data for assignment\n",
    "og_data_02 = og_data.drop(og_data_98.index) # test data to be used by prof\n",
    "#og_data_02.to_csv('data/og_data_02.csv', index=False)  # already created csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5404</th>\n",
       "      <td>science</td>\n",
       "      <td>june 2021 portoro is hosting the 8th european ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6441</th>\n",
       "      <td>unrest</td>\n",
       "      <td>israel has launched an air strike against gaza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>labour</td>\n",
       "      <td>that the covid 19 pandemic has left organisati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6978</th>\n",
       "      <td>weather</td>\n",
       "      <td>tropical storm danny makes south carolina coas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175</th>\n",
       "      <td>weather</td>\n",
       "      <td>weather inshore until 6 pm on wednesday will b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                                               text\n",
       "5404  science  june 2021 portoro is hosting the 8th european ...\n",
       "6441   unrest  israel has launched an air strike against gaza...\n",
       "3364   labour  that the covid 19 pandemic has left organisati...\n",
       "6978  weather  tropical storm danny makes south carolina coas...\n",
       "7175  weather  weather inshore until 6 pm on wednesday will b..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_data_98.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    144\n",
       "text        144\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_data_02.count() # check if 2% is kept aside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5404</th>\n",
       "      <td>science</td>\n",
       "      <td>june 2021 portoro is hosting the 8th european ...</td>\n",
       "      <td>june  portoro host th european congress mathem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6441</th>\n",
       "      <td>unrest</td>\n",
       "      <td>israel has launched an air strike against gaza...</td>\n",
       "      <td>israel launch air strike gaza renew clash spar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3364</th>\n",
       "      <td>labour</td>\n",
       "      <td>that the covid 19 pandemic has left organisati...</td>\n",
       "      <td>covid  pandem left organis struggl secur manag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6978</th>\n",
       "      <td>weather</td>\n",
       "      <td>tropical storm danny makes south carolina coas...</td>\n",
       "      <td>tropic storm danni make south carolina coastal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7175</th>\n",
       "      <td>weather</td>\n",
       "      <td>weather inshore until 6 pm on wednesday will b...</td>\n",
       "      <td>weather inshor  pm wednesday hazi misti foggi ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     category                                               text  \\\n",
       "5404  science  june 2021 portoro is hosting the 8th european ...   \n",
       "6441   unrest  israel has launched an air strike against gaza...   \n",
       "3364   labour  that the covid 19 pandemic has left organisati...   \n",
       "6978  weather  tropical storm danny makes south carolina coas...   \n",
       "7175  weather  weather inshore until 6 pm on wednesday will b...   \n",
       "\n",
       "                                             clean_text  \n",
       "5404  june  portoro host th european congress mathem...  \n",
       "6441  israel launch air strike gaza renew clash spar...  \n",
       "3364  covid  pandem left organis struggl secur manag...  \n",
       "6978  tropic storm danni make south carolina coastal...  \n",
       "7175  weather inshor  pm wednesday hazi misti foggi ...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    # call stop words and remove them\n",
    "    stop_words = stopwords.words('english') \n",
    "    removed_stopwords_text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    # Perform stemming\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    return ' '.join(stemmer.stem(word) for word in removed_stopwords_text.split(' '))\n",
    "\n",
    "\n",
    "og_data_98[\"clean_text\"] = og_data_98[\"text\"].apply(clean_text)\n",
    "og_data_98.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB with Bag of Words accuracy: 0.678\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_nb= og_data_98[\"clean_text\"]\n",
    "y_nb= og_data_98[\"category\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test= train_test_split(X_nb,y_nb,test_size=.2,random_state=42)\n",
    "\n",
    "# Multinomial Naive Bayes with Bag of Words\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"MultinomialNB with Bag of Words accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:[0.66028912 0.66539116 0.6619898 ]\n",
      "Mean Accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "cv_scores = cross_val_score(model, X_nb, y_nb, cv=StratifiedKFold(n_splits=3, shuffle=True), scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-Validation Scores:{cv_scores}\")\n",
    "\n",
    "print(f\"Mean Accuracy: {np.mean(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 Accuracy: 0.9040532879818595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_logistic = le.fit_transform(og_data_98['category'])\n",
    "X_logistic = og_data_98['clean_text']\n",
    "\n",
    "# Create pipeline\n",
    "model2 = make_pipeline(\n",
    "    TfidfVectorizer(max_features=5000),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "# Train\n",
    "model2.fit(X_logistic, y_logistic)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Model 2 Accuracy:\", model2.score(X_logistic, y_logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 77ms/step - accuracy: 0.0873 - loss: 2.8339 - val_accuracy: 0.1841 - val_loss: 2.4538\n",
      "Epoch 2/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 101ms/step - accuracy: 0.2376 - loss: 2.3102 - val_accuracy: 0.3102 - val_loss: 2.2181\n",
      "Epoch 3/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 106ms/step - accuracy: 0.4133 - loss: 1.8017 - val_accuracy: 0.4200 - val_loss: 1.9613\n",
      "Epoch 4/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 108ms/step - accuracy: 0.6544 - loss: 1.1648 - val_accuracy: 0.5127 - val_loss: 1.8607\n",
      "Epoch 5/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.7504 - loss: 0.8245 - val_accuracy: 0.5170 - val_loss: 1.9243\n",
      "Epoch 6/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 106ms/step - accuracy: 0.8581 - loss: 0.5078 - val_accuracy: 0.5078 - val_loss: 2.1642\n",
      "Epoch 7/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.9038 - loss: 0.3559 - val_accuracy: 0.5595 - val_loss: 2.1388\n",
      "Epoch 8/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.9477 - loss: 0.2044 - val_accuracy: 0.5241 - val_loss: 2.3871\n",
      "Epoch 9/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 105ms/step - accuracy: 0.9653 - loss: 0.1472 - val_accuracy: 0.5496 - val_loss: 2.4210\n",
      "Epoch 10/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.9804 - loss: 0.0790 - val_accuracy: 0.5432 - val_loss: 2.6222\n",
      "Epoch 11/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.9794 - loss: 0.0813 - val_accuracy: 0.5163 - val_loss: 2.8572\n",
      "Epoch 12/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.9585 - loss: 0.1494 - val_accuracy: 0.5432 - val_loss: 2.7377\n",
      "Epoch 13/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.9949 - loss: 0.0322 - val_accuracy: 0.5184 - val_loss: 3.0360\n",
      "Epoch 14/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.9724 - loss: 0.1001 - val_accuracy: 0.5290 - val_loss: 2.9350\n",
      "Epoch 15/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.9783 - loss: 0.0945 - val_accuracy: 0.5382 - val_loss: 2.9170\n",
      "Epoch 16/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.9912 - loss: 0.0337 - val_accuracy: 0.5475 - val_loss: 3.0741\n",
      "Epoch 17/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 103ms/step - accuracy: 0.9964 - loss: 0.0150 - val_accuracy: 0.5425 - val_loss: 3.1734\n",
      "Epoch 18/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.9977 - loss: 0.0109 - val_accuracy: 0.5609 - val_loss: 3.2067\n",
      "Epoch 19/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.9982 - loss: 0.0077 - val_accuracy: 0.5588 - val_loss: 3.2594\n",
      "Epoch 20/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 104ms/step - accuracy: 0.9990 - loss: 0.0056 - val_accuracy: 0.5644 - val_loss: 3.2989\n",
      "Model 3 Validation Accuracy: 0.5644475817680359\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(og_data_98['clean_text'])\n",
    "tokenizer.num_words = 5000\n",
    "sequences = tokenizer.texts_to_sequences(og_data_98['clean_text'])\n",
    "X_lstm = pad_sequences(sequences, maxlen=200)\n",
    "y_lstm= le.fit_transform(og_data_98['category'])\n",
    "\n",
    "\n",
    "# Build model\n",
    "model3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(5000, 128),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model3.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model3.fit(\n",
    "    X_lstm, y_lstm,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Model 3 Validation Accuracy:\", max(history.history['val_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import joblib\n",
    "from tensorflow.keras.models import save_model,load_model \n",
    "\n",
    "\n",
    "# Model 1: Original Model (Naive Bayes)\n",
    "joblib.dump(model, 'NaiveBayes.pkl')\n",
    "\n",
    "# Model 2: Logistic Regression\n",
    "joblib.dump(model2, 'logreg_model.pkl')\n",
    "\n",
    "# Model 3: LSTM\n",
    "model3.save('lstm_model.h5')\n",
    "\n",
    "\n",
    "# Loading function for all models\n",
    "def load_all_models():\n",
    "    model1 = joblib.load('original_model.pkl')\n",
    "    model2 = joblib.load('logreg_model.pkl')\n",
    "    model3 = load_model('lstm_model.h5')\n",
    "    \n",
    "    return model1, model2, model3\n",
    "\n",
    "# Prediction function for all models\n",
    "def predict_all_models(text, model1, model2, model3):\n",
    "    cleaned = clean_text(text)\n",
    "    \n",
    "    # Model 1 (Naive Bayes)\n",
    "    pred1 = model1.predict([cleaned])[0]\n",
    "    \n",
    "    # Model 2 (Logistic Regression)\n",
    "    pred2 = model2.predict([cleaned])[0]\n",
    "    \n",
    "    # Model 3 (LSTM)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned])\n",
    "    padded = pad_sequences(sequence, maxlen=200)\n",
    "    pred3 = model3.predict(padded).argmax(axis=1)[0]\n",
    "    \n",
    "    return {\n",
    "        'NaiveBayes': le.inverse_transform([pred1])[0],\n",
    "        'Logistic Regression': le.inverse_transform([pred2])[0],\n",
    "        'LSTM': le.inverse_transform([pred3])[0]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MultinomialNB from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 123ms/step\n",
      "Predictions: {'NaiveBayes': 'arts', 'Logistic Regression': 'arts', 'LSTM': 'labour'}\n"
     ]
    }
   ],
   "source": [
    "# Load models once at startup\n",
    "model1, model2, model3= load_all_models()\n",
    "\n",
    "# Make predictions\n",
    "sample_text = \"Archaeologists discover ancient temple ruins under city center\"\n",
    "predictions = predict_all_models(sample_text, model1, model2, model3)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
