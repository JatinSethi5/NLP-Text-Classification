{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tokenizers import Tokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Sethi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sethi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sethi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data = pd.read_csv(\"data/output_chunk_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data_98 = og_data.sample(frac = 0.98) # work data for assignment\n",
    "og_data_02 = og_data.drop(og_data_98.index) # test data to be used by prof\n",
    "#og_data_02.to_csv('data/og_data_02.csv', index=False)  # already created csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>lifestyle</td>\n",
       "      <td>as more people become vaccinated and borders b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6181</th>\n",
       "      <td>sport</td>\n",
       "      <td>when i dream of fried green tomatoes this is l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>unrest</td>\n",
       "      <td>singapore authorities view far right extremism...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>education</td>\n",
       "      <td>the mission behind the invisible cities in our...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5022</th>\n",
       "      <td>religion</td>\n",
       "      <td>a texas inmate convicted in the december 2009 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                               text\n",
       "3659  lifestyle  as more people become vaccinated and borders b...\n",
       "6181      sport  when i dream of fried green tomatoes this is l...\n",
       "6566     unrest  singapore authorities view far right extremism...\n",
       "1856  education  the mission behind the invisible cities in our...\n",
       "5022   religion  a texas inmate convicted in the december 2009 ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_data_98.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    144\n",
       "text        144\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_data_02.count() # check if 2% is kept aside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3659</th>\n",
       "      <td>lifestyle</td>\n",
       "      <td>as more people become vaccinated and borders b...</td>\n",
       "      <td>peopl becom vaccin border begin open option tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6181</th>\n",
       "      <td>sport</td>\n",
       "      <td>when i dream of fried green tomatoes this is l...</td>\n",
       "      <td>dream fri green tomato liter see photo via han...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6566</th>\n",
       "      <td>unrest</td>\n",
       "      <td>singapore authorities view far right extremism...</td>\n",
       "      <td>singapor author view far right extrem emerg ma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1856</th>\n",
       "      <td>education</td>\n",
       "      <td>the mission behind the invisible cities in our...</td>\n",
       "      <td>mission behind invis citi shoe challeng help s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5022</th>\n",
       "      <td>religion</td>\n",
       "      <td>a texas inmate convicted in the december 2009 ...</td>\n",
       "      <td>texa inmat convict decemb  fatal stab pregnant...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                               text  \\\n",
       "3659  lifestyle  as more people become vaccinated and borders b...   \n",
       "6181      sport  when i dream of fried green tomatoes this is l...   \n",
       "6566     unrest  singapore authorities view far right extremism...   \n",
       "1856  education  the mission behind the invisible cities in our...   \n",
       "5022   religion  a texas inmate convicted in the december 2009 ...   \n",
       "\n",
       "                                             clean_text  \n",
       "3659  peopl becom vaccin border begin open option tr...  \n",
       "6181  dream fri green tomato liter see photo via han...  \n",
       "6566  singapor author view far right extrem emerg ma...  \n",
       "1856  mission behind invis citi shoe challeng help s...  \n",
       "5022  texa inmat convict decemb  fatal stab pregnant...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    # call stop words and remove them\n",
    "    stop_words = stopwords.words('english') \n",
    "    removed_stopwords_text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    # Perform stemming\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    return ' '.join(stemmer.stem(word) for word in removed_stopwords_text.split(' '))\n",
    "\n",
    "\n",
    "og_data_98[\"clean_text\"] = og_data_98[\"text\"].apply(clean_text)\n",
    "og_data_98.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB with Bag of Words accuracy: 0.668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_nb= og_data_98[\"clean_text\"]\n",
    "y_nb= og_data_98[\"category\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test= train_test_split(X_nb,y_nb,test_size=.2,random_state=42)\n",
    "\n",
    "# Multinomial Naive Bayes with Bag of Words\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"MultinomialNB with Bag of Words accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:[0.65986395 0.65008503 0.67984694]\n",
      "Mean Accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "cv_scores = cross_val_score(model, X_nb, y_nb, cv=StratifiedKFold(n_splits=3, shuffle=True), scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-Validation Scores:{cv_scores}\")\n",
    "\n",
    "print(f\"Mean Accuracy: {np.mean(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 Accuracy: 0.9037698412698413\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_logistic = le.fit_transform(og_data_98['category'])\n",
    "X_logistic = og_data_98['clean_text']\n",
    "\n",
    "# Create pipeline\n",
    "model2 = make_pipeline(\n",
    "    TfidfVectorizer(max_features=5000),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "# Train\n",
    "model2.fit(X_logistic, y_logistic)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Model 2 Accuracy:\", model2.score(X_logistic, y_logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 77ms/step - accuracy: 0.0864 - loss: 2.8302 - val_accuracy: 0.2472 - val_loss: 2.4031\n",
      "Epoch 2/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 70ms/step - accuracy: 0.3396 - loss: 2.0497 - val_accuracy: 0.3987 - val_loss: 1.9581\n",
      "Epoch 3/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.5210 - loss: 1.5081 - val_accuracy: 0.4433 - val_loss: 1.9503\n",
      "Epoch 4/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 36ms/step - accuracy: 0.6258 - loss: 1.1596 - val_accuracy: 0.4455 - val_loss: 1.9066\n",
      "Epoch 5/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.7176 - loss: 0.9084 - val_accuracy: 0.4731 - val_loss: 1.9621\n",
      "Epoch 6/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.7945 - loss: 0.6640 - val_accuracy: 0.4993 - val_loss: 2.0777\n",
      "Epoch 7/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.8345 - loss: 0.5188 - val_accuracy: 0.4766 - val_loss: 2.2547\n",
      "Epoch 8/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step - accuracy: 0.8843 - loss: 0.3893 - val_accuracy: 0.4943 - val_loss: 2.4355\n",
      "Epoch 9/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 54ms/step - accuracy: 0.9182 - loss: 0.3009 - val_accuracy: 0.5042 - val_loss: 2.6460\n",
      "Epoch 10/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 70ms/step - accuracy: 0.9474 - loss: 0.2018 - val_accuracy: 0.4724 - val_loss: 2.6823\n",
      "Epoch 11/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 69ms/step - accuracy: 0.9410 - loss: 0.2244 - val_accuracy: 0.5064 - val_loss: 2.7859\n",
      "Epoch 12/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 71ms/step - accuracy: 0.9790 - loss: 0.0852 - val_accuracy: 0.5113 - val_loss: 2.9860\n",
      "Epoch 13/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 53ms/step - accuracy: 0.9555 - loss: 0.1961 - val_accuracy: 0.4986 - val_loss: 3.0987\n",
      "Epoch 14/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.9705 - loss: 0.1100 - val_accuracy: 0.4965 - val_loss: 2.8361\n",
      "Epoch 15/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.9757 - loss: 0.1062 - val_accuracy: 0.5170 - val_loss: 2.8917\n",
      "Epoch 16/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.9876 - loss: 0.0484 - val_accuracy: 0.5262 - val_loss: 3.0095\n",
      "Epoch 17/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.9917 - loss: 0.0303 - val_accuracy: 0.5205 - val_loss: 3.1815\n",
      "Epoch 18/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.9965 - loss: 0.0163 - val_accuracy: 0.5368 - val_loss: 3.2747\n",
      "Epoch 19/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.9975 - loss: 0.0106 - val_accuracy: 0.5347 - val_loss: 3.3271\n",
      "Epoch 20/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 35ms/step - accuracy: 0.9970 - loss: 0.0109 - val_accuracy: 0.5312 - val_loss: 3.3342\n",
      "Model 3 Validation Accuracy: 0.5368272066116333\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(og_data_98['clean_text'])\n",
    "tokenizer.num_words = 5000\n",
    "sequences = tokenizer.texts_to_sequences(og_data_98['clean_text'])\n",
    "X_lstm = pad_sequences(sequences, maxlen=200)\n",
    "y_lstm= le.fit_transform(og_data_98['category'])\n",
    "\n",
    "\n",
    "# Build model\n",
    "model3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(5000, 128),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model3.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model3.fit(\n",
    "    X_lstm, y_lstm,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Model 3 Validation Accuracy:\", max(history.history['val_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels = og_data_98['category'] # Add your actual categories\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "# Save the trained label encoder\n",
    "pickle.dump(label_encoder, open('label_encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import joblib\n",
    "from tensorflow.keras.models import save_model,load_model \n",
    "\n",
    "\n",
    "# Model 1: Original Model (Naive Bayes)\n",
    "joblib.dump(model, 'NaiveBayes.pkl')\n",
    "\n",
    "\n",
    "# Model 2: Logistic Regression\n",
    "joblib.dump(model2, 'logreg_model.pkl')\n",
    "pickle.dump(model2, open('model.pkl','wb'))\n",
    "\n",
    "\n",
    "# Model 3: LSTM\n",
    "model3.save('lstm_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Loading function for all models\n",
    "def load_all_models():\n",
    "    model1 = joblib.load('original_model.pkl')\n",
    "    model2 = joblib.load('logreg_model.pkl')\n",
    "    model3 = load_model('lstm_model.h5')\n",
    "    \n",
    "    return model1, model2, model3\n",
    "\n",
    "# Prediction function for all models\n",
    "def predict_all_models(text, model1, model2, model3):\n",
    "    cleaned = clean_text(text)\n",
    "    \n",
    "    # Model 1 (Naive Bayes)\n",
    "    pred1 = model1.predict([cleaned])[0]\n",
    "    \n",
    "    # Model 2 (Logistic Regression)\n",
    "    pred2 = model2.predict([cleaned])[0]\n",
    "    \n",
    "    # Model 3 (LSTM)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned])\n",
    "    padded = pad_sequences(sequence, maxlen=200)\n",
    "    pred3 = model3.predict(padded).argmax(axis=1)[0]\n",
    "    \n",
    "    return {\n",
    "        'NaiveBayes': le.inverse_transform([pred1])[0],\n",
    "        'Logistic Regression': le.inverse_transform([pred2])[0],\n",
    "        'LSTM': le.inverse_transform([pred3])[0]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 395ms/step\n",
      "Predictions: {'NaiveBayes': 'arts', 'Logistic Regression': 'arts', 'LSTM': 'politics'}\n"
     ]
    }
   ],
   "source": [
    "# Load models once at startup\n",
    "model1, model2, model3= load_all_models()\n",
    "\n",
    "# Make predictions\n",
    "sample_text = \"Archaeologists discover ancient temple ruins under city center\"\n",
    "predictions = predict_all_models(sample_text, model1, model2, model3)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
