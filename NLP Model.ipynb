{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tokenizers import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Sethi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Sethi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Sethi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\o'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\o'\n",
      "C:\\Users\\Sethi\\AppData\\Local\\Temp\\ipykernel_31180\\861610438.py:1: SyntaxWarning: invalid escape sequence '\\o'\n",
      "  og_data = pd.read_csv(\"data\\output_chunk_7.csv\")\n"
     ]
    }
   ],
   "source": [
    "og_data = pd.read_csv(\"data\\output_chunk_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data_98 = og_data.sample(frac = 0.98) # work data for assignment\n",
    "og_data_02 = og_data.drop(og_data_98.index) # test data to be used by prof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6721</th>\n",
       "      <td>unrest</td>\n",
       "      <td>least  peopl kill burkina faso deadliest attac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>environmental</td>\n",
       "      <td>endang hawaiian monk seal recent transport pap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>humanInterest</td>\n",
       "      <td>franc learment  saugeen district senior school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>disaster</td>\n",
       "      <td>uae ministri health prevent friday report  cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>health</td>\n",
       "      <td>gasp air patient haiti infect coronavirus die ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text\n",
       "6721         unrest  least  peopl kill burkina faso deadliest attac...\n",
       "2033  environmental  endang hawaiian monk seal recent transport pap...\n",
       "2925  humanInterest  franc learment  saugeen district senior school...\n",
       "1196       disaster  uae ministri health prevent friday report  cas...\n",
       "2425         health  gasp air patient haiti infect coronavirus die ..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    # call stop words and remove them\n",
    "    stop_words = stopwords.words('english') \n",
    "    removed_stopwords_text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    # Perform stemming\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    return ' '.join(stemmer.stem(word) for word in removed_stopwords_text.split(' '))\n",
    "\n",
    "\n",
    "og_data_test = og_data_98\n",
    "og_data_test[\"text\"] = og_data_98[\"text\"].apply(clean_text)\n",
    "og_data_test.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6721</th>\n",
       "      <td>unrest</td>\n",
       "      <td>least  peopl kill burkina faso deadliest attac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>environmental</td>\n",
       "      <td>endang hawaiian monk seal recent transport pap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>humanInterest</td>\n",
       "      <td>franc learment  saugeen district senior school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>disaster</td>\n",
       "      <td>uae ministri health prevent friday report  cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>health</td>\n",
       "      <td>gasp air patient haiti infect coronavirus die ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text\n",
       "6721         unrest  least  peopl kill burkina faso deadliest attac...\n",
       "2033  environmental  endang hawaiian monk seal recent transport pap...\n",
       "2925  humanInterest  franc learment  saugeen district senior school...\n",
       "1196       disaster  uae ministri health prevent friday report  cas...\n",
       "2425         health  gasp air patient haiti infect coronavirus die ..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_data_98.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6721</th>\n",
       "      <td>unrest</td>\n",
       "      <td>least  peopl kill burkina faso deadliest attac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2033</th>\n",
       "      <td>environmental</td>\n",
       "      <td>endang hawaiian monk seal recent transport pap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2925</th>\n",
       "      <td>humanInterest</td>\n",
       "      <td>franc learment  saugeen district senior school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1196</th>\n",
       "      <td>disaster</td>\n",
       "      <td>uae ministri health prevent friday report  cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2425</th>\n",
       "      <td>health</td>\n",
       "      <td>gasp air patient haiti infect coronavirus die ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           category                                               text\n",
       "6721         unrest  least  peopl kill burkina faso deadliest attac...\n",
       "2033  environmental  endang hawaiian monk seal recent transport pap...\n",
       "2925  humanInterest  franc learment  saugeen district senior school...\n",
       "1196       disaster  uae ministri health prevent friday report  cas...\n",
       "2425         health  gasp air patient haiti infect coronavirus die ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_data_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB with Bag of Words accuracy: 0.672\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X= og_data_test[\"text\"]\n",
    "y= og_data_test[\"category\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test= train_test_split(X,y,test_size=.2,random_state=42)\n",
    "\n",
    "# Multinomial Naive Bayes with Bag of Words\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"MultinomialNB with Bag of Words accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:[0.6704932  0.65561224 0.66369048]\n",
      "Mean Accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "cv_scores = cross_val_score(model, X, y, cv=StratifiedKFold(n_splits=3, shuffle=True), scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-Validation Scores:{cv_scores}\")\n",
    "\n",
    "print(f\"Mean Accuracy: {np.mean(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = transformers.BertTokenizer.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 Accuracy: 0.9030612244897959\n"
     ]
    }
   ],
   "source": [
    "#Start\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(og_data_98['category'])\n",
    "\n",
    "# Create pipeline\n",
    "model2 = make_pipeline(\n",
    "    TfidfVectorizer(max_features=5000),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "# Train\n",
    "model2.fit(og_data_98['text'], y)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Model 2 Accuracy:\", model2.score(og_data_98['text'], y))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 41ms/step - accuracy: 0.0929 - loss: 2.8448 - val_accuracy: 0.2018 - val_loss: 2.5350\n",
      "Epoch 2/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.2433 - loss: 2.3137 - val_accuracy: 0.3831 - val_loss: 1.9933\n",
      "Epoch 3/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.4712 - loss: 1.6856 - val_accuracy: 0.4788 - val_loss: 1.7485\n",
      "Epoch 4/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step - accuracy: 0.6264 - loss: 1.2242 - val_accuracy: 0.4653 - val_loss: 1.8403\n",
      "Epoch 5/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 37ms/step - accuracy: 0.6892 - loss: 1.0148 - val_accuracy: 0.5297 - val_loss: 1.7411\n",
      "Epoch 6/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.7860 - loss: 0.7125 - val_accuracy: 0.5333 - val_loss: 1.9066\n",
      "Epoch 7/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.8607 - loss: 0.4779 - val_accuracy: 0.5375 - val_loss: 2.0017\n",
      "Epoch 8/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.9157 - loss: 0.3120 - val_accuracy: 0.5198 - val_loss: 2.2109\n",
      "Epoch 9/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - accuracy: 0.9352 - loss: 0.2382 - val_accuracy: 0.5184 - val_loss: 2.4206\n",
      "Epoch 10/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - accuracy: 0.9522 - loss: 0.1864 - val_accuracy: 0.5411 - val_loss: 2.4368\n",
      "Epoch 11/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.9745 - loss: 0.1086 - val_accuracy: 0.5057 - val_loss: 2.8066\n",
      "Epoch 12/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.9570 - loss: 0.1649 - val_accuracy: 0.5184 - val_loss: 2.7153\n",
      "Epoch 13/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.9749 - loss: 0.1063 - val_accuracy: 0.5170 - val_loss: 2.9852\n",
      "Epoch 14/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - accuracy: 0.9810 - loss: 0.0754 - val_accuracy: 0.5248 - val_loss: 2.9714\n",
      "Epoch 15/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.9722 - loss: 0.1073 - val_accuracy: 0.5021 - val_loss: 3.0335\n",
      "Epoch 16/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 38ms/step - accuracy: 0.9819 - loss: 0.0736 - val_accuracy: 0.5007 - val_loss: 3.2238\n",
      "Epoch 17/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - accuracy: 0.9706 - loss: 0.1302 - val_accuracy: 0.5035 - val_loss: 3.0858\n",
      "Epoch 18/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.9870 - loss: 0.0585 - val_accuracy: 0.5113 - val_loss: 3.2217\n",
      "Epoch 19/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.9943 - loss: 0.0267 - val_accuracy: 0.5241 - val_loss: 3.0940\n",
      "Epoch 20/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 63ms/step - accuracy: 0.9921 - loss: 0.0301 - val_accuracy: 0.5234 - val_loss: 3.3459\n",
      "Model 3 Validation Accuracy: 0.5410764813423157\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "og_data_98['cleaned_text'] = og_data_98['text'].apply(preprocess_text)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(og_data_98['cleaned_text'])\n",
    "tokenizer.num_words = 5000\n",
    "sequences = tokenizer.texts_to_sequences(og_data_98['cleaned_text'])\n",
    "X = pad_sequences(sequences, maxlen=200)\n",
    "\n",
    "\n",
    "# Build model\n",
    "model3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(5000, 128),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model3.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model3.fit(\n",
    "    X, y,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Model 3 Validation Accuracy:\", max(history.history['val_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 43ms/step - accuracy: 0.0733 - loss: 2.8645 - val_accuracy: 0.1629 - val_loss: 2.6396\n",
      "Epoch 2/10\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.2012 - loss: 2.4685 - val_accuracy: 0.2493 - val_loss: 2.3583\n",
      "Epoch 3/10\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.3177 - loss: 2.0920 - val_accuracy: 0.2854 - val_loss: 2.2513\n",
      "Epoch 4/10\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 42ms/step - accuracy: 0.4614 - loss: 1.6980 - val_accuracy: 0.3414 - val_loss: 2.1650\n",
      "Epoch 5/10\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.6103 - loss: 1.2459 - val_accuracy: 0.3683 - val_loss: 2.1650\n",
      "Epoch 6/10\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 41ms/step - accuracy: 0.7282 - loss: 0.8957 - val_accuracy: 0.4108 - val_loss: 2.2827\n",
      "Epoch 7/10\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - accuracy: 0.8413 - loss: 0.5470 - val_accuracy: 0.4207 - val_loss: 2.4244\n",
      "Epoch 8/10\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - accuracy: 0.9119 - loss: 0.3365 - val_accuracy: 0.4306 - val_loss: 2.8050\n",
      "Epoch 9/10\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 39ms/step - accuracy: 0.9356 - loss: 0.2341 - val_accuracy: 0.4200 - val_loss: 3.1186\n",
      "Epoch 10/10\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 40ms/step - accuracy: 0.9592 - loss: 0.1654 - val_accuracy: 0.4377 - val_loss: 3.1582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# First create and save all models\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential, save_model, load_model\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Load and preprocess data\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'<.*?>+', '', text)\n",
    "    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub(r'\\n', '', text)\n",
    "    text = re.sub(r'\\w*\\d\\w*', '', text)\n",
    "    return text\n",
    "\n",
    "og_data = pd.read_csv(\"data/output_chunk_7.csv\")\n",
    "og_data['cleaned_text'] = og_data['text'].apply(preprocess_text)\n",
    "\n",
    "# Split data\n",
    "og_data_98 = og_data.sample(frac=0.98)\n",
    "og_data_02 = og_data.drop(og_data_98.index)\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(og_data_98['category'])\n",
    "\n",
    "# Model 1: Original Model (Naive Bayes)\n",
    "model1 = make_pipeline(\n",
    "    TfidfVectorizer(max_features=5000),\n",
    "    MultinomialNB()\n",
    ")\n",
    "model1.fit(og_data_98['cleaned_text'], y)\n",
    "joblib.dump(model1, 'NaiveBayes.pkl')\n",
    "\n",
    "# Model 2: Logistic Regression\n",
    "model2 = make_pipeline(\n",
    "    TfidfVectorizer(max_features=5000),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "model2.fit(og_data_98['cleaned_text'], y)\n",
    "joblib.dump(model2, 'logreg_model.pkl')\n",
    "\n",
    "# Model 3: LSTM\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(og_data_98['cleaned_text'])\n",
    "sequences = tokenizer.texts_to_sequences(og_data_98['cleaned_text'])\n",
    "X_lstm = pad_sequences(sequences, maxlen=200)\n",
    "\n",
    "model3 = Sequential([\n",
    "    Embedding(5000, 128),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "model3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model3.fit(X_lstm, y, epochs=10, validation_split=0.2)\n",
    "model3.save('lstm_model.h5')\n",
    "\n",
    "# Save preprocessing artifacts\n",
    "joblib.dump(le, 'label_encoder.pkl')\n",
    "with open('tokenizer.json', 'w') as f:\n",
    "    f.write(tokenizer.to_json())\n",
    "\n",
    "# Loading function for all models\n",
    "def load_all_models():\n",
    "    model1 = joblib.load('original_model.pkl')\n",
    "    model2 = joblib.load('logreg_model.pkl')\n",
    "    model3 = load_model('lstm_model.h5')\n",
    "    le = joblib.load('label_encoder.pkl')\n",
    "    \n",
    "    with open('tokenizer.json', 'r') as f:\n",
    "        tokenizer = tokenizer_from_json(f.read())\n",
    "    \n",
    "    return model1, model2, model3, le, tokenizer\n",
    "\n",
    "# Prediction function for all models\n",
    "def predict_all_models(text, model1, model2, model3, le, tokenizer):\n",
    "    cleaned = preprocess_text(text)\n",
    "    \n",
    "    # Model 1 (Naive Bayes)\n",
    "    pred1 = model1.predict([cleaned])[0]\n",
    "    \n",
    "    # Model 2 (Logistic Regression)\n",
    "    pred2 = model2.predict([cleaned])[0]\n",
    "    \n",
    "    # Model 3 (LSTM)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned])\n",
    "    padded = pad_sequences(sequence, maxlen=200)\n",
    "    pred3 = model3.predict(padded).argmax(axis=1)[0]\n",
    "    \n",
    "    return {\n",
    "        'NaiveBayes': le.inverse_transform([pred1])[0],\n",
    "        'Logistic Regression': le.inverse_transform([pred2])[0],\n",
    "        'LSTM': le.inverse_transform([pred3])[0]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\n",
      "Predictions: {'NaiveBayes': 'arts', 'Logistic Regression': 'arts', 'LSTM': 'science'}\n"
     ]
    }
   ],
   "source": [
    "# Load models once at startup\n",
    "model1, model2, model3, le, tokenizer = load_all_models()\n",
    "\n",
    "# Make predictions\n",
    "sample_text = \"Archaeologists discover ancient temple ruins under city center\"\n",
    "predictions = predict_all_models(sample_text, model1, model2, model3, le, tokenizer)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
