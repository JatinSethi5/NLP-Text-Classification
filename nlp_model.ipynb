{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from tokenizers import Tokenizer\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/soumyekumar/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/soumyekumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/soumyekumar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "og_data = pd.read_csv(\"data/output_chunk_7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "og_data_98, og_data_02 = train_test_split(og_data,test_size=0.02,stratify=og_data['category'],  random_state=42)\n",
    "#og_data_02.to_csv('data/og_data_02.csv', index=False)  # already created csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original distribution:\n",
      " category\n",
      "arts             400\n",
      "crime            400\n",
      "unrest           400\n",
      "sport            400\n",
      "social           400\n",
      "science          400\n",
      "religion         400\n",
      "politics         400\n",
      "other            400\n",
      "lifestyle        400\n",
      "labour           400\n",
      "humanInterest    400\n",
      "health           400\n",
      "environmental    400\n",
      "education        400\n",
      "economy          400\n",
      "disaster         400\n",
      "weather          400\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Original distribution:\\n\", og_data['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "98% subset:\n",
      " category\n",
      "social           392\n",
      "politics         392\n",
      "disaster         392\n",
      "labour           392\n",
      "education        392\n",
      "weather          392\n",
      "science          392\n",
      "unrest           392\n",
      "environmental    392\n",
      "other            392\n",
      "religion         392\n",
      "sport            392\n",
      "crime            392\n",
      "arts             392\n",
      "humanInterest    392\n",
      "lifestyle        392\n",
      "economy          392\n",
      "health           392\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n98% subset:\\n\", og_data_98['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2% subset:\n",
      " category\n",
      "religion         8\n",
      "humanInterest    8\n",
      "labour           8\n",
      "unrest           8\n",
      "science          8\n",
      "arts             8\n",
      "politics         8\n",
      "sport            8\n",
      "crime            8\n",
      "lifestyle        8\n",
      "economy          8\n",
      "environmental    8\n",
      "disaster         8\n",
      "other            8\n",
      "social           8\n",
      "education        8\n",
      "health           8\n",
      "weather          8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n2% subset:\\n\", og_data_02['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5955</th>\n",
       "      <td>social</td>\n",
       "      <td>andrew cuomo s ex girlfriend sandra lee issued...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>politics</td>\n",
       "      <td>dubai united arab emirates ap the outgoing chi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4616</th>\n",
       "      <td>politics</td>\n",
       "      <td>centre for independent studies executive direc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>economy</td>\n",
       "      <td>valletta malta prnewswire today at the extraor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>lifestyle</td>\n",
       "      <td>frequent flyers on star alliance member airlin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                               text\n",
       "5955     social  andrew cuomo s ex girlfriend sandra lee issued...\n",
       "4753   politics  dubai united arab emirates ap the outgoing chi...\n",
       "4616   politics  centre for independent studies executive direc...\n",
       "1434    economy  valletta malta prnewswire today at the extraor...\n",
       "3799  lifestyle  frequent flyers on star alliance member airlin..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_data_98.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "category    144\n",
       "text        144\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "og_data_02.count() # check if 2% is kept aside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5955</th>\n",
       "      <td>social</td>\n",
       "      <td>andrew cuomo s ex girlfriend sandra lee issued...</td>\n",
       "      <td>andrew cuomo ex girlfriend sandra lee issu wor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4753</th>\n",
       "      <td>politics</td>\n",
       "      <td>dubai united arab emirates ap the outgoing chi...</td>\n",
       "      <td>dubai unit arab emir ap outgo chief israel mos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4616</th>\n",
       "      <td>politics</td>\n",
       "      <td>centre for independent studies executive direc...</td>\n",
       "      <td>centr independ studi execut director tom switz...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>economy</td>\n",
       "      <td>valletta malta prnewswire today at the extraor...</td>\n",
       "      <td>valletta malta prnewswir today extraordinari g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3799</th>\n",
       "      <td>lifestyle</td>\n",
       "      <td>frequent flyers on star alliance member airlin...</td>\n",
       "      <td>frequent flyer star allianc member airlin soon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       category                                               text  \\\n",
       "5955     social  andrew cuomo s ex girlfriend sandra lee issued...   \n",
       "4753   politics  dubai united arab emirates ap the outgoing chi...   \n",
       "4616   politics  centre for independent studies executive direc...   \n",
       "1434    economy  valletta malta prnewswire today at the extraor...   \n",
       "3799  lifestyle  frequent flyers on star alliance member airlin...   \n",
       "\n",
       "                                             clean_text  \n",
       "5955  andrew cuomo ex girlfriend sandra lee issu wor...  \n",
       "4753  dubai unit arab emir ap outgo chief israel mos...  \n",
       "4616  centr independ studi execut director tom switz...  \n",
       "1434  valletta malta prnewswir today extraordinari g...  \n",
       "3799  frequent flyer star allianc member airlin soon...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
    "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
    "    text = BeautifulSoup(text, \"html.parser\").get_text()  # Remove HTML tags\n",
    "    # call stop words and remove them\n",
    "    stop_words = stopwords.words('english') \n",
    "    removed_stopwords_text = ' '.join(word for word in text.split(' ') if word not in stop_words)\n",
    "    # Perform stemming\n",
    "    stemmer = nltk.SnowballStemmer(\"english\")\n",
    "    return ' '.join(stemmer.stem(word) for word in removed_stopwords_text.split(' '))\n",
    "\n",
    "\n",
    "og_data_98[\"clean_text\"] = og_data_98[\"text\"].apply(clean_text)\n",
    "og_data_98.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB with Bag of Words accuracy: 0.676\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_nb= og_data_98[\"clean_text\"]\n",
    "y_nb= og_data_98[\"category\"]\n",
    "\n",
    "X_train,X_test,y_train,y_test= train_test_split(X_nb,y_nb,test_size=.2,random_state=42)\n",
    "\n",
    "# Multinomial Naive Bayes with Bag of Words\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "\n",
    "model = make_pipeline(CountVectorizer(), MultinomialNB())\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"MultinomialNB with Bag of Words accuracy: {accuracy:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Scores:[0.67517007 0.67176871 0.65858844]\n",
      "Mean Accuracy: 0.67\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "cv_scores = cross_val_score(model, X_nb, y_nb, cv=StratifiedKFold(n_splits=3, shuffle=True), scoring='accuracy')\n",
    "\n",
    "print(f\"Cross-Validation Scores:{cv_scores}\")\n",
    "\n",
    "print(f\"Mean Accuracy: {np.mean(cv_scores):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_attention_masks=False, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 2 Accuracy: 0.903202947845805\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_logistic = le.fit_transform(og_data_98['category'])\n",
    "X_logistic = og_data_98['clean_text']\n",
    "\n",
    "# Create pipeline\n",
    "model2 = make_pipeline(\n",
    "    TfidfVectorizer(max_features=5000),\n",
    "    LogisticRegression(max_iter=1000)\n",
    ")\n",
    "\n",
    "# Train\n",
    "model2.fit(X_logistic, y_logistic)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Model 2 Accuracy:\", model2.score(X_logistic, y_logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 82ms/step - accuracy: 0.0859 - loss: 2.8174 - val_accuracy: 0.2160 - val_loss: 2.3785\n",
      "Epoch 2/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 106ms/step - accuracy: 0.2492 - loss: 2.2676 - val_accuracy: 0.2783 - val_loss: 2.1702\n",
      "Epoch 3/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 109ms/step - accuracy: 0.3969 - loss: 1.8353 - val_accuracy: 0.3477 - val_loss: 2.0556\n",
      "Epoch 4/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 110ms/step - accuracy: 0.5310 - loss: 1.4754 - val_accuracy: 0.4348 - val_loss: 1.9119\n",
      "Epoch 5/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 117ms/step - accuracy: 0.6441 - loss: 1.1698 - val_accuracy: 0.4525 - val_loss: 2.0109\n",
      "Epoch 6/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 109ms/step - accuracy: 0.7528 - loss: 0.8270 - val_accuracy: 0.5113 - val_loss: 2.0726\n",
      "Epoch 7/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 106ms/step - accuracy: 0.7868 - loss: 0.7410 - val_accuracy: 0.4979 - val_loss: 2.1439\n",
      "Epoch 8/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 105ms/step - accuracy: 0.8735 - loss: 0.4442 - val_accuracy: 0.5014 - val_loss: 2.3495\n",
      "Epoch 9/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 107ms/step - accuracy: 0.9168 - loss: 0.3189 - val_accuracy: 0.4979 - val_loss: 2.5668\n",
      "Epoch 10/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 108ms/step - accuracy: 0.9080 - loss: 0.3451 - val_accuracy: 0.4894 - val_loss: 2.6735\n",
      "Epoch 11/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 115ms/step - accuracy: 0.9491 - loss: 0.2116 - val_accuracy: 0.4958 - val_loss: 2.8596\n",
      "Epoch 12/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 110ms/step - accuracy: 0.9692 - loss: 0.1273 - val_accuracy: 0.5099 - val_loss: 3.0041\n",
      "Epoch 13/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 109ms/step - accuracy: 0.9781 - loss: 0.0919 - val_accuracy: 0.4780 - val_loss: 3.2283\n",
      "Epoch 14/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 109ms/step - accuracy: 0.9425 - loss: 0.2026 - val_accuracy: 0.4710 - val_loss: 2.9136\n",
      "Epoch 15/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 110ms/step - accuracy: 0.9370 - loss: 0.2302 - val_accuracy: 0.5050 - val_loss: 3.0299\n",
      "Epoch 16/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 111ms/step - accuracy: 0.9746 - loss: 0.0996 - val_accuracy: 0.4865 - val_loss: 3.1764\n",
      "Epoch 17/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 114ms/step - accuracy: 0.9881 - loss: 0.0608 - val_accuracy: 0.4958 - val_loss: 3.4118\n",
      "Epoch 18/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 113ms/step - accuracy: 0.9870 - loss: 0.0637 - val_accuracy: 0.4950 - val_loss: 3.5079\n",
      "Epoch 19/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 114ms/step - accuracy: 0.9957 - loss: 0.0335 - val_accuracy: 0.5106 - val_loss: 3.5284\n",
      "Epoch 20/20\n",
      "\u001b[1m177/177\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 112ms/step - accuracy: 0.9961 - loss: 0.0207 - val_accuracy: 0.5205 - val_loss: 3.5306\n",
      "Model 3 Validation Accuracy: 0.5205382704734802\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(og_data_98['clean_text'])\n",
    "tokenizer.num_words = 5000\n",
    "sequences = tokenizer.texts_to_sequences(og_data_98['clean_text'])\n",
    "X_lstm = pad_sequences(sequences, maxlen=200)\n",
    "y_lstm= le.fit_transform(og_data_98['category'])\n",
    "\n",
    "\n",
    "# Build model\n",
    "model3 = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(5000, 128),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(le.classes_), activation='softmax')\n",
    "])\n",
    "\n",
    "model3.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history = model3.fit(\n",
    "    X_lstm, y_lstm,\n",
    "    batch_size=32,\n",
    "    epochs=20,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "print(\"Model 3 Validation Accuracy:\", max(history.history['val_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "labels = og_data_98['category'] # Add your actual categories\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(labels)\n",
    "\n",
    "# Save the trained label encoder\n",
    "pickle.dump(label_encoder, open('label_encoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import joblib\n",
    "from tensorflow.keras.models import save_model,load_model \n",
    "\n",
    "\n",
    "# Model 1: Original Model (Naive Bayes)\n",
    "joblib.dump(model, 'NaiveBayes.pkl')\n",
    "\n",
    "\n",
    "# Model 2: Logistic Regression\n",
    "joblib.dump(model2, 'logreg_model.pkl')\n",
    "pickle.dump(model2, open('model.pkl','wb'))\n",
    "\n",
    "\n",
    "# Model 3: LSTM\n",
    "model3.save('lstm_model.h5')\n",
    "\n",
    "\n",
    "\n",
    "# Loading function for all models\n",
    "def load_all_models():\n",
    "    model1 = joblib.load('original_model.pkl')\n",
    "    model2 = joblib.load('logreg_model.pkl')\n",
    "    model3 = load_model('lstm_model.h5')\n",
    "    \n",
    "    return model1, model2, model3\n",
    "\n",
    "# Prediction function for all models\n",
    "def predict_all_models(text, model1, model2, model3):\n",
    "    cleaned = clean_text(text)\n",
    "    \n",
    "    # Model 1 (Naive Bayes)\n",
    "    pred1 = model1.predict([cleaned])[0]\n",
    "    \n",
    "    # Model 2 (Logistic Regression)\n",
    "    pred2 = model2.predict([cleaned])[0]\n",
    "    \n",
    "    # Model 3 (LSTM)\n",
    "    sequence = tokenizer.texts_to_sequences([cleaned])\n",
    "    padded = pad_sequences(sequence, maxlen=200)\n",
    "    pred3 = model3.predict(padded).argmax(axis=1)[0]\n",
    "    \n",
    "    return {\n",
    "        'NaiveBayes': le.inverse_transform([pred1])[0],\n",
    "        'Logistic Regression': le.inverse_transform([pred2])[0],\n",
    "        'LSTM': le.inverse_transform([pred3])[0]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfTransformer from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator TfidfVectorizer from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator MultinomialNB from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py:376: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.4.2 when using version 1.5.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 122ms/step\n",
      "Predictions: {'NaiveBayes': 'arts', 'Logistic Regression': 'arts', 'LSTM': 'other'}\n"
     ]
    }
   ],
   "source": [
    "# Load models once at startup\n",
    "model1, model2, model3= load_all_models()\n",
    "\n",
    "# Make predictions\n",
    "sample_text = \"Archaeologists discover ancient temple ruins under city center\"\n",
    "predictions = predict_all_models(sample_text, model1, model2, model3)\n",
    "\n",
    "print(\"Predictions:\", predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
